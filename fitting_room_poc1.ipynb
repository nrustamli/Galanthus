{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nrustamli/Galanthus/blob/main/fitting_room_poc1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7eM0GThF09D"
      },
      "source": [
        "# ü™û The Fitting Room - AI Proof of Concept\n",
        "\n",
        "**Goal:** Generate a photorealistic image of a specific person wearing a \"White T-shirt and Blue Jeans\" while preserving facial identity and body shape.\n",
        "\n",
        "**Stack:**\n",
        "- Base Model: SDXL 1.0 (Stable Diffusion XL)\n",
        "- Identity Adapter: IP-Adapter-FaceID (uses InsightFace embeddings for face preservation)\n",
        "- Pose Control: ControlNet OpenPose\n",
        "- Face Analysis: InsightFace (antelopev2 model)\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Google Colab Setup\n",
        "\n",
        "**Before running:** Go to `Runtime` ‚Üí `Change runtime type` ‚Üí Select **T4 GPU**\n",
        "\n",
        "---\n",
        "\n",
        "## Phase 1: Environment Setup\n",
        "\n",
        "Run this cell to install all required dependencies:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5HXUmetF09E"
      },
      "outputs": [],
      "source": [
        "# ‚Ññ Install all required dependencies (Google Colab)\n",
        "# Upgrade pip first for better dependency resolution\n",
        "!pip install -q --upgrade pip\n",
        "\n",
        "# Uninstall existing packages to prevent conflicts\n",
        "!pip uninstall -y mediapipe opencv-python controlnet-aux\n",
        "\n",
        "# Core diffusers and transformers\n",
        "!pip install -q diffusers[torch]>=0.25.0 transformers>=4.36.0 accelerate>=0.25.0 peft>=0.7.0\n",
        "\n",
        "# Image processing and pose detection\n",
        "!pip install -q opencv-python>=4.8.0 # Install opencv-python first\n",
        "!pip install -q mediapipe              # Then install latest mediapipe\n",
        "!pip install -q controlnet-aux>=0.0.7 Pillow>=10.0.0 # Then other libraries\n",
        "\n",
        "# InsightFace for face analysis (CRITICAL for IP-Adapter-FaceID)\n",
        "!pip install -q insightface>=0.7.3 onnxruntime-gpu>=1.16.0\n",
        "\n",
        "# Utilities\n",
        "!pip install -q huggingface-hub>=0.20.0 safetensors>=0.4.0 matplotlib numpy>=1.24.0\n",
        "!pip install -q gdown  # For downloading models from Google Drive\n",
        "\n",
        "print(\"‚úÖ All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lp_VN6FLF09F"
      },
      "outputs": [],
      "source": [
        "# üî• Download ALL Required Models for the Fitting Room\n",
        "# This cell downloads and caches all models needed for the project\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from huggingface_hub import hf_hub_download, snapshot_download\n",
        "\n",
        "# =============================================================================\n",
        "# 1Ô∏è‚É£ INSIGHTFACE MODELS (Face Analysis & Embedding)\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"1Ô∏è‚É£ DOWNLOADING INSIGHTFACE MODELS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "INSIGHTFACE_ROOT = os.path.expanduser(\"~/.insightface/models\")\n",
        "os.makedirs(INSIGHTFACE_ROOT, exist_ok=True)\n",
        "\n",
        "def download_insightface_model(model_name, urls, target_dir):\n",
        "    \"\"\"Download InsightFace model with fallback URLs.\"\"\"\n",
        "    model_dir = os.path.join(target_dir, model_name)\n",
        "\n",
        "    # Check if model already exists\n",
        "    if os.path.exists(model_dir):\n",
        "        files = os.listdir(model_dir) if os.path.isdir(model_dir) else []\n",
        "        if len(files) >= 4:\n",
        "            print(f\"   ‚úÖ {model_name} already exists ({len(files)} files)\")\n",
        "            return True\n",
        "\n",
        "    print(f\"üì• Downloading {model_name} model...\")\n",
        "\n",
        "    for i, url in enumerate(urls):\n",
        "        try:\n",
        "            print(f\"   Trying source {i+1}/{len(urls)}...\")\n",
        "            zip_path = f\"/tmp/{model_name}.zip\"\n",
        "\n",
        "            # Download\n",
        "            result = subprocess.run(\n",
        "                [\"wget\", \"-q\", \"--show-progress\", url, \"-O\", zip_path],\n",
        "                capture_output=True, text=True, timeout=300\n",
        "            )\n",
        "\n",
        "            if result.returncode != 0:\n",
        "                raise Exception(f\"wget failed: {result.stderr}\")\n",
        "\n",
        "            # Check if file was downloaded\n",
        "            if not os.path.exists(zip_path) or os.path.getsize(zip_path) < 1000:\n",
        "                raise Exception(\"Downloaded file is too small or missing\")\n",
        "\n",
        "            # Unzip\n",
        "            subprocess.run([\"unzip\", \"-q\", \"-o\", zip_path, \"-d\", target_dir], check=True)\n",
        "\n",
        "            # Cleanup\n",
        "            os.remove(zip_path)\n",
        "\n",
        "            print(f\"   ‚úÖ {model_name} downloaded successfully!\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è Source {i+1} failed: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"   ‚ùå Failed to download {model_name} from all sources\")\n",
        "    return False\n",
        "\n",
        "# ---- Download antelopev2 (REQUIRED for IP-Adapter-FaceID) ----\n",
        "antelopev2_urls = [\n",
        "    \"https://huggingface.co/MonsterMMORPG/tools/resolve/main/antelopev2.zip\",\n",
        "    \"https://huggingface.co/DIAMONIK7777/antelopev2/resolve/main/antelopev2.zip\",\n",
        "    \"https://huggingface.co/ashleykleynhans/inswapper/resolve/main/antelopev2.zip\",\n",
        "]\n",
        "download_insightface_model(\"antelopev2\", antelopev2_urls, INSIGHTFACE_ROOT)\n",
        "\n",
        "# ---- Download buffalo_l (Full face analysis suite) ----\n",
        "buffalo_urls = [\n",
        "    \"https://huggingface.co/datasets/Gourieff/ReActor/resolve/main/models/insightface/buffalo_l.zip\",\n",
        "    \"https://huggingface.co/deepinsight/insightface/resolve/main/models/buffalo_l.zip\",\n",
        "]\n",
        "download_insightface_model(\"buffalo_l\", buffalo_urls, INSIGHTFACE_ROOT)\n",
        "\n",
        "# ---- Download buffalo_sc (Smaller, faster model) ----\n",
        "buffalo_sc_urls = [\n",
        "    \"https://huggingface.co/datasets/Gourieff/ReActor/resolve/main/models/insightface/buffalo_sc.zip\",\n",
        "]\n",
        "download_insightface_model(\"buffalo_sc\", buffalo_sc_urls, INSIGHTFACE_ROOT)\n",
        "\n",
        "# Verify InsightFace installation\n",
        "print(f\"\\nüìÅ InsightFace models location: {INSIGHTFACE_ROOT}\")\n",
        "print(\"\\nüìã Installed models:\")\n",
        "for model in os.listdir(INSIGHTFACE_ROOT):\n",
        "    model_path = os.path.join(INSIGHTFACE_ROOT, model)\n",
        "    if os.path.isdir(model_path):\n",
        "        files = os.listdir(model_path)\n",
        "        print(f\"   üì¶ {model}: {len(files)} files\")\n",
        "        for f in files[:5]:  # Show first 5 files\n",
        "            print(f\"      - {f}\")\n",
        "        if len(files) > 5:\n",
        "            print(f\"      ... and {len(files) - 5} more\")\n",
        "\n",
        "# =============================================================================\n",
        "# 2Ô∏è‚É£ IP-ADAPTER MODELS (Identity Preservation)\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"2Ô∏è‚É£ DOWNLOADING IP-ADAPTER MODELS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create models directory\n",
        "MODELS_DIR = Path(\"./models\")\n",
        "MODELS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# IP-Adapter Face models for SDXL\n",
        "print(\"üì• Downloading IP-Adapter-Plus-Face for SDXL...\")\n",
        "ip_adapter_face_path = hf_hub_download(\n",
        "    repo_id=\"h94/IP-Adapter\",\n",
        "    filename=\"sdxl_models/ip-adapter-plus-face_sdxl_vit-h.safetensors\",\n",
        "    local_dir=MODELS_DIR / \"ip-adapter\"\n",
        ")\n",
        "print(f\"   ‚úÖ IP-Adapter-Plus-Face: {ip_adapter_face_path}\")\n",
        "\n",
        "# IP-Adapter-FaceID for SDXL (uses InsightFace embeddings directly - BETTER!)\n",
        "print(\"üì• Downloading IP-Adapter-FaceID for SDXL (recommended)...\")\n",
        "try:\n",
        "    ip_adapter_faceid_path = hf_hub_download(\n",
        "        repo_id=\"h94/IP-Adapter-FaceID\",\n",
        "        filename=\"ip-adapter-faceid_sdxl.bin\",\n",
        "        local_dir=MODELS_DIR / \"ip-adapter-faceid\"\n",
        "    )\n",
        "    print(f\"   ‚úÖ IP-Adapter-FaceID: {ip_adapter_faceid_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è Could not download IP-Adapter-FaceID: {e}\")\n",
        "\n",
        "# IP-Adapter-FaceID-Plus (combines face embedding + face image)\n",
        "print(\"üì• Downloading IP-Adapter-FaceID-Plus for SDXL (best quality)...\")\n",
        "try:\n",
        "    ip_adapter_faceid_plus_path = hf_hub_download(\n",
        "        repo_id=\"h94/IP-Adapter-FaceID\",\n",
        "        filename=\"ip-adapter-faceid-plusv2_sdxl.bin\",\n",
        "        local_dir=MODELS_DIR / \"ip-adapter-faceid\"\n",
        "    )\n",
        "    print(f\"   ‚úÖ IP-Adapter-FaceID-Plus: {ip_adapter_faceid_plus_path}\")\n",
        "\n",
        "    # Download the LoRA weights for FaceID-Plus\n",
        "    ip_adapter_faceid_lora_path = hf_hub_download(\n",
        "        repo_id=\"h94/IP-Adapter-FaceID\",\n",
        "        filename=\"ip-adapter-faceid-plusv2_sdxl_lora.safetensors\",\n",
        "        local_dir=MODELS_DIR / \"ip-adapter-faceid\"\n",
        "    )\n",
        "    print(f\"   ‚úÖ IP-Adapter-FaceID-Plus LoRA: {ip_adapter_faceid_lora_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è Could not download IP-Adapter-FaceID-Plus: {e}\")\n",
        "\n",
        "# Download image encoder for IP-Adapter\n",
        "print(\"üì• Downloading CLIP Image Encoder (for IP-Adapter)...\")\n",
        "try:\n",
        "    snapshot_download(\n",
        "        repo_id=\"h94/IP-Adapter\",\n",
        "        allow_patterns=[\"sdxl_models/image_encoder/*\"],\n",
        "        local_dir=MODELS_DIR / \"ip-adapter\"\n",
        "    )\n",
        "    print(\"   ‚úÖ CLIP Image Encoder downloaded!\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è Could not download image encoder: {e}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 3Ô∏è‚É£ CONTROLNET MODELS (Pose Control)\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"3Ô∏è‚É£ DOWNLOADING CONTROLNET MODELS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ControlNet OpenPose for SDXL\n",
        "print(\"üì• Pre-caching ControlNet OpenPose for SDXL...\")\n",
        "try:\n",
        "    from diffusers import ControlNetModel\n",
        "    controlnet_pose = ControlNetModel.from_pretrained(\n",
        "        \"thibaud/controlnet-openpose-sdxl-1.0\",\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        cache_dir=MODELS_DIR / \"controlnet\"\n",
        "    )\n",
        "    del controlnet_pose  # Free memory after caching\n",
        "    print(\"   ‚úÖ ControlNet OpenPose cached!\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è Could not pre-cache ControlNet: {e}\")\n",
        "    print(\"      (Will be downloaded on first use)\")\n",
        "\n",
        "# =============================================================================\n",
        "# 4Ô∏è‚É£ OPENPOSE MODELS (for controlnet-aux)\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"4Ô∏è‚É£ PRE-CACHING OPENPOSE DETECTOR\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"üì• Pre-caching OpenPose detector...\")\n",
        "try:\n",
        "    from controlnet_aux import OpenposeDetector\n",
        "    openpose = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\")\n",
        "    del openpose  # Free memory after caching\n",
        "    print(\"   ‚úÖ OpenPose detector cached!\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è Could not pre-cache OpenPose: {e}\")\n",
        "\n",
        "# =============================================================================\n",
        "# üìä SUMMARY\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìä MODEL DOWNLOAD SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "‚úÖ InsightFace Models (Face Analysis):\n",
        "   - antelopev2: Face detection + embedding (for IP-Adapter-FaceID)\n",
        "   - buffalo_l: Full suite (age, gender, expression, embedding)\n",
        "\n",
        "‚úÖ IP-Adapter Models (Identity Preservation):\n",
        "   - ip-adapter-plus-face_sdxl: Standard face adapter\n",
        "   - ip-adapter-faceid_sdxl: Uses InsightFace embeddings (recommended)\n",
        "   - ip-adapter-faceid-plusv2_sdxl: Best quality (face + embedding)\n",
        "\n",
        "‚úÖ ControlNet Models (Pose Control):\n",
        "   - controlnet-openpose-sdxl: Body pose control\n",
        "\n",
        "‚úÖ Auxiliary Models:\n",
        "   - OpenPose detector: Pose extraction\n",
        "   - CLIP image encoder: For IP-Adapter\n",
        "\n",
        "üöÄ All models are ready! Proceed to the next cells.\n",
        "\"\"\")\n",
        "\n",
        "print(f\"\\nüìÅ Models cached in: {MODELS_DIR.resolve()}\")\n",
        "!du -sh {MODELS_DIR}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5Rc7cdRF09G"
      },
      "outputs": [],
      "source": [
        "# üß™ Verify InsightFace Installation\n",
        "# This cell tests that InsightFace models are properly installed and working\n",
        "\n",
        "import os  # IMPORTANT: ensure os is imported\n",
        "from insightface.app import FaceAnalysis\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üß™ VERIFYING INSIGHTFACE INSTALLATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Define the InsightFace root directory\n",
        "INSIGHTFACE_ROOT = os.path.expanduser(\"~/.insightface\")\n",
        "\n",
        "# Test loading antelopev2 model (REQUIRED for IP-Adapter-FaceID)\n",
        "print(\"\\nüì¶ Testing antelopev2 model (required for face embeddings)...\")\n",
        "try:\n",
        "    app_antelope = FaceAnalysis(\n",
        "        name='antelopev2',\n",
        "        root=INSIGHTFACE_ROOT,\n",
        "        providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
        "    )\n",
        "    app_antelope.prepare(ctx_id=0, det_size=(640, 640))\n",
        "    print(\"   ‚úÖ antelopev2 loaded successfully!\")\n",
        "    print(f\"   üìä Models loaded: {list(app_antelope.models.keys())}\")\n",
        "\n",
        "    # Verify the model has the required components\n",
        "    required_models = ['detection', 'recognition']\n",
        "    for model_type in required_models:\n",
        "        if model_type in app_antelope.models:\n",
        "            print(f\"   ‚úì {model_type} model available\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è {model_type} model missing - this may cause issues\")\n",
        "\n",
        "    del app_antelope\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Failed to load antelopev2: {e}\")\n",
        "    print(\"   üí° Run Cell 2 to download the required models\")\n",
        "\n",
        "# Test loading buffalo_l model (alternative model)\n",
        "print(\"\\nüì¶ Testing buffalo_l model (alternative face analysis suite)...\")\n",
        "try:\n",
        "    app_buffalo = FaceAnalysis(\n",
        "        name='buffalo_l',\n",
        "        root=INSIGHTFACE_ROOT,\n",
        "        providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
        "    )\n",
        "    app_buffalo.prepare(ctx_id=0, det_size=(640, 640))\n",
        "    print(\"   ‚úÖ buffalo_l loaded successfully!\")\n",
        "    print(f\"   üìä Models loaded: {list(app_buffalo.models.keys())}\")\n",
        "    del app_buffalo\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è buffalo_l not available: {e}\")\n",
        "    print(\"   (This is optional - antelopev2 is sufficient)\")\n",
        "\n",
        "# Create a test image to verify face detection works\n",
        "print(\"\\nüîç Testing face detection with a dummy image...\")\n",
        "try:\n",
        "    # Create a simple test image (blank)\n",
        "    test_image = np.zeros((640, 640, 3), dtype=np.uint8)\n",
        "\n",
        "    app_test = FaceAnalysis(\n",
        "        name='antelopev2',\n",
        "        root=INSIGHTFACE_ROOT,\n",
        "        providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
        "    )\n",
        "    app_test.prepare(ctx_id=0, det_size=(640, 640))\n",
        "\n",
        "    # Run detection (should return empty list for blank image)\n",
        "    faces = app_test.get(test_image)\n",
        "    print(f\"   ‚úÖ Face detection working! (Found {len(faces)} faces in test image)\")\n",
        "    del app_test\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Face detection test failed: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ INSIGHTFACE VERIFICATION COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "üìã InsightFace provides the following capabilities:\n",
        "   - Face Detection (RetinaFace/SCRFD) - locates faces in images\n",
        "   - Face Recognition (ArcFace embeddings) - 512-dimensional identity vector\n",
        "   - Face Landmarks (2D & 3D) - 5 keypoints for alignment\n",
        "   - Face Attributes (age, gender) - demographic analysis\n",
        "\n",
        "üîó The 512-dim face embeddings from ArcFace are used by IP-Adapter-FaceID\n",
        "   for identity preservation in image generation.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mSK01u7F09G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "# Check CUDA availability\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Pp3PhZCF09H"
      },
      "source": [
        "## Phase 2: Upload Your Images\n",
        "\n",
        "We need two inputs:\n",
        "1. **Face Image (selfie)**: A clear photo of your face for identity preservation\n",
        "2. **Pose Image (full body)**: A full-body photo for body shape and pose reference\n",
        "\n",
        "**Run the cell below to upload your images:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZ4mF593F09H"
      },
      "outputs": [],
      "source": [
        "# üì§ Set up your images\n",
        "# Choose between Google Colab upload or Local file paths\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# ============================================================\n",
        "# CONFIGURATION: Set this based on your environment\n",
        "# ============================================================\n",
        "USE_COLAB = True  # Set to True for Google Colab, False for local development\n",
        "\n",
        "if USE_COLAB:\n",
        "    # Google Colab: Upload images interactively\n",
        "    from google.colab import files\n",
        "    import io\n",
        "\n",
        "    print(\"üì∏ Please upload your FACE IMAGE (selfie):\")\n",
        "    uploaded_face = files.upload()\n",
        "    FACE_IMAGE_NAME = list(uploaded_face.keys())[0]\n",
        "    print(f\"   ‚úÖ Uploaded: {FACE_IMAGE_NAME}\")\n",
        "\n",
        "    print(\"\\nüì∏ Please upload your POSE IMAGE (full body):\")\n",
        "    uploaded_pose = files.upload()\n",
        "    POSE_IMAGE_NAME = list(uploaded_pose.keys())[0]\n",
        "    print(f\"   ‚úÖ Uploaded: {POSE_IMAGE_NAME}\")\n",
        "\n",
        "    # Save uploaded files\n",
        "    with open(FACE_IMAGE_NAME, 'wb') as f:\n",
        "        f.write(uploaded_face[FACE_IMAGE_NAME])\n",
        "    with open(POSE_IMAGE_NAME, 'wb') as f:\n",
        "        f.write(uploaded_pose[POSE_IMAGE_NAME])\n",
        "\n",
        "    FACE_IMAGE_PATH = Path(FACE_IMAGE_NAME)\n",
        "    POSE_IMAGE_PATH = Path(POSE_IMAGE_NAME)\n",
        "else:\n",
        "    # Local Development: Use pre-existing images in the project\n",
        "    # Images are located in the parent directory of proof_of_concept\n",
        "    PROJECT_ROOT = Path(\"..\").resolve()  # Go up from proof_of_concept\n",
        "\n",
        "    # Available images in the project:\n",
        "    # - selfie.jpg (for face/identity)\n",
        "    # - full_body.jpeg (for pose reference)\n",
        "    # - closer_body_and_face.jpeg (alternative)\n",
        "\n",
        "    FACE_IMAGE_PATH = PROJECT_ROOT / \"selfie.jpg\"\n",
        "    POSE_IMAGE_PATH = PROJECT_ROOT / \"full_body.jpeg\"\n",
        "\n",
        "    # Verify files exist\n",
        "    if not FACE_IMAGE_PATH.exists():\n",
        "        raise FileNotFoundError(f\"Face image not found: {FACE_IMAGE_PATH}\")\n",
        "    if not POSE_IMAGE_PATH.exists():\n",
        "        raise FileNotFoundError(f\"Pose image not found: {POSE_IMAGE_PATH}\")\n",
        "\n",
        "    print(f\"üìÇ Using local images from: {PROJECT_ROOT}\")\n",
        "\n",
        "# Output directory\n",
        "OUTPUT_DIR = Path(\"./outputs\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"üìã IMAGE CONFIGURATION\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"üë§ Face image: {FACE_IMAGE_PATH}\")\n",
        "print(f\"ü¶¥ Pose image: {POSE_IMAGE_PATH}\")\n",
        "print(f\"üìÅ Output dir: {OUTPUT_DIR.resolve()}\")\n",
        "print(f\"üñ•Ô∏è Environment: {'Google Colab' if USE_COLAB else 'Local Development'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAh0QxLXF09H"
      },
      "outputs": [],
      "source": [
        "from diffusers.utils import load_image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the input images\n",
        "face_image = load_image(str(FACE_IMAGE_PATH))\n",
        "pose_image = load_image(str(POSE_IMAGE_PATH))\n",
        "\n",
        "# Display the input images\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "axes[0].imshow(face_image)\n",
        "axes[0].set_title(\"Face Image (Identity Source)\")\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(pose_image)\n",
        "axes[1].set_title(\"Pose Image (Body Shape Source)\")\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmQfvBrRF09H"
      },
      "source": [
        "## Phase 3: Extract Face Embeddings (InstantID)\n",
        "\n",
        "Using InsightFace's `antelopev2` model to extract facial features that will preserve identity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yXLLNpNF09H"
      },
      "outputs": [],
      "source": [
        "# üîç Initialize InsightFace for Face Embedding Extraction\n",
        "# The antelopev2 model provides ArcFace embeddings used by IP-Adapter-FaceID\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from insightface.app import FaceAnalysis\n",
        "\n",
        "# Define paths\n",
        "INSIGHTFACE_ROOT = os.path.expanduser(\"~/.insightface\")\n",
        "model_dir = os.path.join(INSIGHTFACE_ROOT, \"models\", \"antelopev2\")\n",
        "\n",
        "# Download antelopev2 model if not present\n",
        "if not os.path.exists(model_dir) or len(os.listdir(model_dir)) < 4:\n",
        "    print(\"üì• Downloading antelopev2 model from HuggingFace...\")\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    # Try multiple sources for reliability\n",
        "    download_urls = [\n",
        "        \"https://huggingface.co/MonsterMMORPG/tools/resolve/main/antelopev2.zip\",\n",
        "        \"https://huggingface.co/DIAMONIK7777/antelopev2/resolve/main/antelopev2.zip\",\n",
        "    ]\n",
        "\n",
        "    success = False\n",
        "    for url in download_urls:\n",
        "        try:\n",
        "            print(f\"   Trying: {url.split('/')[-2]}...\")\n",
        "            !wget -q --show-progress \"{url}\" -O /tmp/antelopev2.zip\n",
        "            !unzip -q -o /tmp/antelopev2.zip -d {INSIGHTFACE_ROOT}/models/\n",
        "            !rm /tmp/antelopev2.zip\n",
        "            success = True\n",
        "            print(\"   ‚úÖ Model downloaded successfully!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è Failed: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not success:\n",
        "        raise RuntimeError(\"‚ùå Could not download antelopev2 model. Please download manually.\")\n",
        "else:\n",
        "    print(f\"‚úÖ antelopev2 model already exists at: {model_dir}\")\n",
        "\n",
        "# Initialize InsightFace FaceAnalysis\n",
        "print(\"\\nüîß Loading InsightFace model...\")\n",
        "app = FaceAnalysis(\n",
        "    name='antelopev2',\n",
        "    root=INSIGHTFACE_ROOT,\n",
        "    providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
        ")\n",
        "app.prepare(ctx_id=0, det_size=(640, 640))\n",
        "\n",
        "# Verify the model loaded correctly\n",
        "print(\"‚úÖ InsightFace model loaded!\")\n",
        "print(f\"   üìä Available models: {list(app.models.keys())}\")\n",
        "print(f\"   üéØ Detection size: 640x640\")\n",
        "print(f\"   üí° This model will extract 512-dim face embeddings for identity preservation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOELRGXsF09I"
      },
      "outputs": [],
      "source": [
        "# üîç Extract Face Embedding from the Selfie\n",
        "# InsightFace extracts a 512-dimensional ArcFace embedding that captures facial identity\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert PIL image to OpenCV format (BGR)\n",
        "face_image_cv2 = cv2.cvtColor(np.array(face_image), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "# Run face detection and analysis\n",
        "print(\"üîç Detecting faces in the selfie...\")\n",
        "face_info_list = app.get(face_image_cv2)\n",
        "\n",
        "if len(face_info_list) == 0:\n",
        "    raise ValueError(\"\"\"\n",
        "‚ùå No face detected in the selfie image!\n",
        "\n",
        "Possible causes:\n",
        "1. Face is too small in the image\n",
        "2. Face is partially occluded\n",
        "3. Lighting is too dark/bright\n",
        "4. Image resolution is too low\n",
        "\n",
        "Solutions:\n",
        "- Use a clear, well-lit selfie\n",
        "- Ensure face takes up at least 20% of the image\n",
        "- Face should be looking directly at the camera\n",
        "\"\"\")\n",
        "\n",
        "print(f\"   Found {len(face_info_list)} face(s) in image\")\n",
        "\n",
        "# Get the largest face (in case multiple faces detected)\n",
        "# Sorting by face bounding box area (width * height)\n",
        "face_info = sorted(\n",
        "    face_info_list,\n",
        "    key=lambda x: (x.bbox[2] - x.bbox[0]) * (x.bbox[3] - x.bbox[1])\n",
        ")[-1]\n",
        "\n",
        "# Extract the embedding (512-dimensional vector)\n",
        "face_emb = face_info.embedding  # numpy array of shape (512,)\n",
        "\n",
        "# Extract face keypoints (5 points: left_eye, right_eye, nose, left_mouth, right_mouth)\n",
        "face_kps = face_info.kps  # numpy array of shape (5, 2)\n",
        "\n",
        "# Extract bounding box for visualization\n",
        "face_bbox = face_info.bbox.astype(int)  # [x1, y1, x2, y2]\n",
        "\n",
        "print(f\"\\n‚úÖ Face embedding extracted successfully!\")\n",
        "print(f\"   üìä Embedding shape: {face_emb.shape} (512-dim ArcFace vector)\")\n",
        "print(f\"   üìç Keypoints shape: {face_kps.shape} (5 facial landmarks)\")\n",
        "print(f\"   üìê Face bounding box: {face_bbox}\")\n",
        "\n",
        "# Visualize the detected face with landmarks\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Original image\n",
        "axes[0].imshow(face_image)\n",
        "axes[0].set_title(\"Original Selfie\", fontsize=12)\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Image with face detection overlay\n",
        "face_image_annotated = np.array(face_image).copy()\n",
        "\n",
        "# Draw bounding box\n",
        "cv2.rectangle(\n",
        "    face_image_annotated,\n",
        "    (face_bbox[0], face_bbox[1]),\n",
        "    (face_bbox[2], face_bbox[3]),\n",
        "    (0, 255, 0), 3\n",
        ")\n",
        "\n",
        "# Draw keypoints\n",
        "colors = [(255, 0, 0), (255, 0, 0), (0, 255, 0), (0, 0, 255), (0, 0, 255)]\n",
        "labels = ['L Eye', 'R Eye', 'Nose', 'L Mouth', 'R Mouth']\n",
        "for i, (kp, color) in enumerate(zip(face_kps, colors)):\n",
        "    cv2.circle(face_image_annotated, (int(kp[0]), int(kp[1])), 5, color, -1)\n",
        "\n",
        "axes[1].imshow(face_image_annotated)\n",
        "axes[1].set_title(\"Detected Face + Landmarks\", fontsize=12)\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° The 512-dim face embedding captures your unique facial identity.\")\n",
        "print(\"   This will be used by IP-Adapter-FaceID to preserve your face in generated images.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25CAAPf5F09I"
      },
      "source": [
        "## Phase 4: Extract Body Pose (OpenPose)\n",
        "\n",
        "Convert the full-body photo into a skeleton/pose map so the AI knows body positioning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVt4gw3WF09I"
      },
      "outputs": [],
      "source": [
        "from controlnet_aux import OpenposeDetector\n",
        "\n",
        "# Initialize OpenPose detector\n",
        "print(\"Loading OpenPose detector...\")\n",
        "openpose = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\")\n",
        "print(\"OpenPose detector loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qxfe2_KQF09I"
      },
      "outputs": [],
      "source": [
        "# Extract pose from full body image\n",
        "pose_map = openpose(pose_image, include_body=True, include_hand=False, include_face=False)\n",
        "\n",
        "# Display the pose map\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 8))\n",
        "axes[0].imshow(pose_image)\n",
        "axes[0].set_title(\"Original Full Body\")\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(pose_map)\n",
        "axes[1].set_title(\"Extracted Pose Map\")\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save pose map for reference\n",
        "pose_map.save(OUTPUT_DIR / \"pose_map.png\")\n",
        "print(f\"‚úÖ Pose map saved to {OUTPUT_DIR / 'pose_map.png'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mdl0sspmF09I"
      },
      "source": [
        "## Phase 5: Load the Generation Pipeline\n",
        "\n",
        "Load the SDXL base model with ControlNet for pose control and IP-Adapter for identity preservation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "housYCOoF09I"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel\n",
        "\n",
        "# Determine device and dtype\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Using dtype: {dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJI7aFEvF09I"
      },
      "outputs": [],
      "source": [
        "# Load ControlNet for OpenPose (body pose)\n",
        "print(\"Loading ControlNet for OpenPose...\")\n",
        "controlnet_pose = ControlNetModel.from_pretrained(\n",
        "    \"thibaud/controlnet-openpose-sdxl-1.0\",\n",
        "    torch_dtype=dtype\n",
        ")\n",
        "print(\"‚úÖ ControlNet for pose loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jll5pquQF09J"
      },
      "outputs": [],
      "source": [
        "# Load the main SDXL Pipeline with ControlNet\n",
        "print(\"Loading SDXL Pipeline with ControlNet (this may take a few minutes)...\")\n",
        "pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    controlnet=controlnet_pose,\n",
        "    torch_dtype=dtype,\n",
        "    variant=\"fp16\" if dtype == torch.float16 else None\n",
        ").to(device)\n",
        "print(\"‚úÖ SDXL Pipeline loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2krsHLkJF09J"
      },
      "outputs": [],
      "source": [
        "# üìã IP-Adapter Loading Options\n",
        "#\n",
        "# This cell initializes flags for IP-Adapter loading.\n",
        "# The actual loading happens in the next cell which uses InsightFace-detected face.\n",
        "#\n",
        "# IP-Adapter Options for SDXL:\n",
        "# 1. ip-adapter-plus-face_sdxl_vit-h.safetensors - Optimized for face images\n",
        "# 2. ip-adapter-faceid_sdxl.bin - Uses InsightFace embeddings directly (requires projection)\n",
        "#\n",
        "# We use option 1 because it's more stable and works well with InsightFace-cropped faces.\n",
        "\n",
        "print(\"üìã IP-Adapter Configuration\")\n",
        "print(\"=\" * 50)\n",
        "print(\"\"\"\n",
        "IP-Adapter will be loaded with the face region detected by InsightFace.\n",
        "\n",
        "Why use InsightFace + IP-Adapter together?\n",
        "1. InsightFace detects and crops the face region accurately\n",
        "2. InsightFace extracts 512-dim embeddings for identity analysis\n",
        "3. IP-Adapter uses the cropped face image for generation conditioning\n",
        "4. This combination provides better identity preservation than full-image input\n",
        "\n",
        "The IP-Adapter will be loaded in the next cell...\n",
        "\"\"\")\n",
        "\n",
        "# Initialize flags\n",
        "IP_ADAPTER_LOADED = False\n",
        "IP_ADAPTER_TYPE = None\n",
        "IP_ADAPTER_SCALE = 0.6\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1p_M2T1F09J"
      },
      "source": [
        "### üîß Loading IP-Adapter with InsightFace Face Detection\n",
        "\n",
        "This cell loads IP-Adapter-Plus-Face and uses the face region detected by InsightFace.\n",
        "\n",
        "**How InsightFace improves identity preservation:**\n",
        "1. **Accurate Face Detection**: InsightFace's SCRFD detector precisely locates faces\n",
        "2. **Face Cropping**: We crop the detected face region with padding for context\n",
        "3. **Embedding Reference**: The 512-dim ArcFace embedding can be used for validation\n",
        "4. **Better Input**: IP-Adapter receives a clean, centered face image instead of full photo\n",
        "\n",
        "**Technical Note:** While IP-Adapter-FaceID can use raw InsightFace embeddings directly,\n",
        "we use IP-Adapter-Plus-Face with InsightFace-cropped images for better stability with SDXL.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8dzkJrWF09J"
      },
      "outputs": [],
      "source": [
        "# üÜï Load IP-Adapter-FaceID for BETTER identity preservation\n",
        "# This uses InsightFace face embeddings directly (from antelopev2 model)\n",
        "#\n",
        "# IP-Adapter-FaceID is specifically trained to work with InsightFace embeddings\n",
        "# and provides better identity preservation than standard IP-Adapter\n",
        "#\n",
        "# ‚ö†Ô∏è NOTE: For IP-Adapter-FaceID with SDXL, we use the standard IP-Adapter-Plus-Face\n",
        "#    because the FaceID SDXL version requires additional projection layers.\n",
        "#    The face embedding will be converted to a face image representation.\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üîß LOADING IP-ADAPTER FOR IDENTITY PRESERVATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Strategy: Use IP-Adapter-Plus-Face which works well with face images\n",
        "# We'll use the face image (cropped from the detected face) for conditioning\n",
        "\n",
        "USE_FACEID_APPROACH = True  # Use face-focused approach\n",
        "\n",
        "if USE_FACEID_APPROACH:\n",
        "    print(\"\\nüì• Loading IP-Adapter-Plus-Face for SDXL...\")\n",
        "\n",
        "    try:\n",
        "        # Load IP-Adapter-Plus-Face (optimized for face images)\n",
        "        pipe.load_ip_adapter(\n",
        "            \"h94/IP-Adapter\",\n",
        "            subfolder=\"sdxl_models\",\n",
        "            weight_name=\"ip-adapter-plus-face_sdxl_vit-h.safetensors\"\n",
        "        )\n",
        "\n",
        "        # Set IP-Adapter scale (controls identity strength vs prompt adherence)\n",
        "        # 0.5-0.7 is a good balance; higher = more identity, less outfit accuracy\n",
        "        IP_ADAPTER_SCALE = 0.6\n",
        "        pipe.set_ip_adapter_scale(IP_ADAPTER_SCALE)\n",
        "\n",
        "        IP_ADAPTER_LOADED = True\n",
        "        IP_ADAPTER_TYPE = \"Plus-Face\"\n",
        "\n",
        "        print(\"‚úÖ IP-Adapter-Plus-Face loaded successfully!\")\n",
        "        print(f\"   üìä IP-Adapter Scale: {IP_ADAPTER_SCALE}\")\n",
        "        print(\"   üí° This adapter is optimized for face identity preservation\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Could not load IP-Adapter: {e}\")\n",
        "        IP_ADAPTER_LOADED = False\n",
        "        IP_ADAPTER_TYPE = None\n",
        "        print(\"\\n‚ö†Ô∏è Proceeding without identity preservation (pose-only mode)\")\n",
        "\n",
        "# Prepare the face image for IP-Adapter conditioning\n",
        "# We'll crop the face region detected by InsightFace for better results\n",
        "if IP_ADAPTER_LOADED:\n",
        "    print(\"\\nüñºÔ∏è Preparing face image for IP-Adapter...\")\n",
        "\n",
        "    # Get the face bounding box (with some padding for context)\n",
        "    x1, y1, x2, y2 = face_bbox\n",
        "\n",
        "    # Add 30% padding around the face\n",
        "    face_width = x2 - x1\n",
        "    face_height = y2 - y1\n",
        "    padding_x = int(face_width * 0.3)\n",
        "    padding_y = int(face_height * 0.3)\n",
        "\n",
        "    # Ensure we don't go out of image bounds\n",
        "    img_width, img_height = face_image.size\n",
        "    x1_padded = max(0, x1 - padding_x)\n",
        "    y1_padded = max(0, y1 - padding_y)\n",
        "    x2_padded = min(img_width, x2 + padding_x)\n",
        "    y2_padded = min(img_height, y2 + padding_y)\n",
        "\n",
        "    # Crop the face region\n",
        "    face_cropped = face_image.crop((x1_padded, y1_padded, x2_padded, y2_padded))\n",
        "\n",
        "    # Resize to 224x224 (IP-Adapter expected size)\n",
        "    face_for_ip_adapter = face_cropped.resize((224, 224))\n",
        "\n",
        "    print(f\"   ‚úÖ Face cropped and resized to 224x224\")\n",
        "    print(f\"   üìê Original crop region: ({x1_padded}, {y1_padded}) to ({x2_padded}, {y2_padded})\")\n",
        "\n",
        "    # Display the cropped face\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(face_for_ip_adapter)\n",
        "    plt.title(\"Face for IP-Adapter\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìä IP-ADAPTER CONFIGURATION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"   Type: {IP_ADAPTER_TYPE if IP_ADAPTER_LOADED else 'None'}\")\n",
        "print(f\"   Loaded: {'‚úÖ Yes' if IP_ADAPTER_LOADED else '‚ùå No'}\")\n",
        "print(f\"   Scale: {IP_ADAPTER_SCALE if IP_ADAPTER_LOADED else 'N/A'}\")\n",
        "print(f\"\\nüí° The face embedding from InsightFace helps identify who to preserve,\")\n",
        "print(\"   while the cropped face image provides visual reference for the adapter.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcrhqNURF09J"
      },
      "outputs": [],
      "source": [
        "# Enable memory optimizations for GPU\n",
        "if device == \"cuda\":\n",
        "    try:\n",
        "        pipe.enable_model_cpu_offload()\n",
        "        print(\"‚úÖ Model CPU offload enabled for memory optimization\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not enable CPU offload: {e}\")\n",
        "\n",
        "    # Uncomment if you have xformers installed for faster inference\n",
        "    # pipe.enable_xformers_memory_efficient_attention()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU3k7oncF09J"
      },
      "source": [
        "## Phase 6: Generate the Image! üé®\n",
        "\n",
        "Now we combine everything to generate the person wearing \"White T-Shirt + Blue Jeans\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4SDk2_2F09J"
      },
      "outputs": [],
      "source": [
        "# The Prompt defining the outfit\n",
        "prompt = \"\"\"A photorealistic full-body shot of a young woman wearing a plain white cotton t-shirt,\n",
        "dark blue denim jeans, white sneakers, standing in a bright modern studio with neutral background,\n",
        "natural lighting, 8k resolution, highly detailed, professional photography, masterpiece\"\"\"\n",
        "\n",
        "negative_prompt = \"\"\"deformed, ugly, bad anatomy, cartoon, drawing, messy, blurry, low quality,\n",
        "bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs,\n",
        "missing arms, missing legs, mutated hands, poorly drawn face, poorly drawn hands\"\"\"\n",
        "\n",
        "print(\"‚úÖ Prompts configured!\")\n",
        "print(f\"\\nüìù Positive prompt:\\n{prompt[:150]}...\")\n",
        "print(f\"\\nüö´ Negative prompt:\\n{negative_prompt[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYgXQOAGF09J"
      },
      "outputs": [],
      "source": [
        "# üìê Resize images to match generation dimensions\n",
        "generation_width = 768\n",
        "generation_height = 1024  # Portrait orientation for full-body\n",
        "\n",
        "# Resize pose map to generation size\n",
        "pose_map_resized = pose_map.resize((generation_width, generation_height))\n",
        "\n",
        "# Note: face_for_ip_adapter was already prepared in the IP-Adapter loading cell\n",
        "# It's the InsightFace-cropped face region resized to 224x224\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"üìê IMAGE DIMENSIONS CONFIGURED\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"\\nüé® Generation size: {generation_width}x{generation_height} (portrait)\")\n",
        "print(f\"ü¶¥ Pose map resized to: {pose_map_resized.size}\")\n",
        "print(f\"üë§ Face image for IP-Adapter: {face_for_ip_adapter.size if 'face_for_ip_adapter' in dir() else 'Not prepared yet'}\")\n",
        "\n",
        "# Show the resized pose map\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
        "axes[0].imshow(pose_map_resized)\n",
        "axes[0].set_title(f\"Pose Map ({generation_width}x{generation_height})\")\n",
        "axes[0].axis('off')\n",
        "\n",
        "if 'face_for_ip_adapter' in dir():\n",
        "    axes[1].imshow(face_for_ip_adapter)\n",
        "    axes[1].set_title(\"Face for IP-Adapter (224x224)\")\n",
        "    axes[1].axis('off')\n",
        "else:\n",
        "    axes[1].text(0.5, 0.5, \"Face not prepared\", ha='center', va='center')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46QbB6KJF09J"
      },
      "outputs": [],
      "source": [
        "# üéõÔ∏è Generation Parameters - TUNE THESE FOR BEST RESULTS\n",
        "# This cell configures all parameters for the image generation\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üéõÔ∏è CONFIGURING GENERATION PARAMETERS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Core generation config\n",
        "generation_config = {\n",
        "    \"prompt\": prompt,\n",
        "    \"negative_prompt\": negative_prompt,\n",
        "    \"image\": pose_map_resized,  # ControlNet condition (pose skeleton)\n",
        "    \"controlnet_conditioning_scale\": 0.8,  # Pose control strength (0.5-1.0)\n",
        "    \"num_inference_steps\": 30,  # Quality vs speed tradeoff (20-50)\n",
        "    \"guidance_scale\": 7.5,  # How strictly to follow prompt (5-15)\n",
        "    \"width\": generation_width,\n",
        "    \"height\": generation_height,\n",
        "    \"generator\": torch.Generator(device=device).manual_seed(42),  # Reproducibility\n",
        "}\n",
        "\n",
        "# Add IP-Adapter conditioning for identity preservation\n",
        "if IP_ADAPTER_LOADED:\n",
        "    # Use the InsightFace-detected and cropped face image\n",
        "    # This provides better identity preservation than using the full selfie\n",
        "    generation_config[\"ip_adapter_image\"] = face_for_ip_adapter\n",
        "\n",
        "    print(f\"\\n‚úÖ IP-Adapter configured with InsightFace-cropped face\")\n",
        "    print(f\"   üìä IP-Adapter Type: {IP_ADAPTER_TYPE}\")\n",
        "    print(f\"   üìä IP-Adapter Scale: {IP_ADAPTER_SCALE}\")\n",
        "    print(f\"   üîó Face embedding shape: {face_emb.shape} (for reference)\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è IP-Adapter not loaded - proceeding with pose-only generation\")\n",
        "    print(\"   Identity may not be preserved without IP-Adapter\")\n",
        "\n",
        "# Display parameter summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìä GENERATION PARAMETER SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\"\"\n",
        "üé® Image Generation:\n",
        "   - Width: {generation_width}px\n",
        "   - Height: {generation_height}px\n",
        "   - Inference Steps: {generation_config['num_inference_steps']}\n",
        "   - Guidance Scale: {generation_config['guidance_scale']}\n",
        "\n",
        "ü¶¥ Pose Control (ControlNet):\n",
        "   - Conditioning Scale: {generation_config['controlnet_conditioning_scale']}\n",
        "   - Pose extracted from: pose_image\n",
        "\n",
        "üë§ Identity Preservation (IP-Adapter):\n",
        "   - Enabled: {'‚úÖ Yes' if IP_ADAPTER_LOADED else '‚ùå No'}\n",
        "   - Type: {IP_ADAPTER_TYPE if IP_ADAPTER_LOADED else 'N/A'}\n",
        "   - Face source: InsightFace-cropped region\n",
        "\n",
        "üé≤ Reproducibility:\n",
        "   - Seed: 42\n",
        "\"\"\")\n",
        "\n",
        "# Tuning tips\n",
        "print(\"=\" * 60)\n",
        "print(\"üí° TUNING TIPS\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "If results need improvement, adjust these parameters:\n",
        "\n",
        "1. IDENTITY TOO WEAK (face doesn't look like input):\n",
        "   ‚Üí Increase IP-Adapter scale: pipe.set_ip_adapter_scale(0.7-0.9)\n",
        "\n",
        "2. OUTFIT IGNORED (wrong clothes):\n",
        "   ‚Üí Decrease IP-Adapter scale: pipe.set_ip_adapter_scale(0.4-0.5)\n",
        "   ‚Üí Increase guidance_scale to 9-12\n",
        "\n",
        "3. POSE INCORRECT (body position wrong):\n",
        "   ‚Üí Increase controlnet_conditioning_scale to 0.9-1.0\n",
        "\n",
        "4. QUALITY ISSUES (blurry/artifacts):\n",
        "   ‚Üí Increase num_inference_steps to 40-50\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88s1uSkoF09K"
      },
      "outputs": [],
      "source": [
        "# üé® Run the generation!\n",
        "print(\"üé® Starting image generation...\")\n",
        "print(\"   This may take 15-60 seconds depending on your GPU...\")\n",
        "print()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "result = pipe(**generation_config)\n",
        "generated_image = result.images[0]\n",
        "\n",
        "generation_time = time.time() - start_time\n",
        "print(f\"\\n‚úÖ Generation complete in {generation_time:.2f} seconds!\")\n",
        "\n",
        "# Check if meets the 20-second criteria\n",
        "if generation_time < 20:\n",
        "    print(\"   üèÜ SUCCESS: Meets the 20-second performance target!\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è Note: Generation took {generation_time:.0f}s (target: <20s on T4 GPU)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ff9yfa0YF09K"
      },
      "outputs": [],
      "source": [
        "# Display the result comparison\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 10))\n",
        "\n",
        "axes[0].imshow(face_image)\n",
        "axes[0].set_title(\"Input: Face (Identity)\", fontsize=14)\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(pose_image)\n",
        "axes[1].set_title(\"Input: Pose Reference\", fontsize=14)\n",
        "axes[1].axis('off')\n",
        "\n",
        "axes[2].imshow(generated_image)\n",
        "axes[2].set_title(\"Output: Generated (White Tee + Jeans)\", fontsize=14)\n",
        "axes[2].axis('off')\n",
        "\n",
        "plt.suptitle(\"ü™û The Fitting Room - AI Virtual Try-On Result\", fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / \"comparison.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìä Comparison saved to: {OUTPUT_DIR / 'comparison.png'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG1h4r_vF09K"
      },
      "outputs": [],
      "source": [
        "# üíæ Save the generated image\n",
        "output_path = OUTPUT_DIR / \"result.png\"\n",
        "generated_image.save(output_path)\n",
        "print(f\"üñºÔ∏è Result saved to: {output_path.resolve()}\")\n",
        "\n",
        "# Download for Google Colab users\n",
        "if USE_COLAB:\n",
        "    from google.colab import files\n",
        "    files.download(str(output_path))\n",
        "    print(\"üì• Download started!\")\n",
        "else:\n",
        "    print(f\"\\nüìÇ Image saved locally at: {output_path.resolve()}\")\n",
        "    print(\"   Open this path in your file browser to view the result.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R629PCoeF09K"
      },
      "source": [
        "## Phase 7: Tuning & Success Metrics\n",
        "\n",
        "### üéõÔ∏è Tuning Guide\n",
        "\n",
        "1. **If the face looks like a caricature:**\n",
        "   - Reduce `ip_adapter_scale` from 0.6 to 0.4\n",
        "   \n",
        "2. **If the clothes are ignored:**\n",
        "   - Increase `guidance_scale` from 7.5 to 9 or 10\n",
        "   - Make prompt more descriptive\n",
        "   \n",
        "3. **If the body shape is wrong:**\n",
        "   - Increase `controlnet_conditioning_scale` to 0.9 or 1.0\n",
        "   - Ensure pose image has correct aspect ratio\n",
        "\n",
        "### ‚úÖ Success Criteria\n",
        "- [ ] The generated face is recognizable as the input user\n",
        "- [ ] The outfit is consistently White T-Shirt + Jeans\n",
        "- [ ] The generation time is under 20 seconds on a T4 GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ3fx80wF09K"
      },
      "outputs": [],
      "source": [
        "# üé≤ Helper function to generate variations with different parameters\n",
        "def generate_variation(seed: int, outfit_prompt: str = None, ip_scale: float = 0.6,\n",
        "                       controlnet_scale: float = 0.8, guidance: float = 7.5):\n",
        "    \"\"\"\n",
        "    Generate a variation with different parameters.\n",
        "\n",
        "    Args:\n",
        "        seed: Random seed for reproducibility\n",
        "        outfit_prompt: Custom prompt for different outfits\n",
        "        ip_scale: IP-Adapter scale (0.0-1.0, controls identity strength)\n",
        "        controlnet_scale: ControlNet scale (0.0-1.0, controls pose adherence)\n",
        "        guidance: Guidance scale (5-15, controls prompt adherence)\n",
        "\n",
        "    Returns:\n",
        "        PIL Image of the generated result\n",
        "    \"\"\"\n",
        "    # Copy the base configuration\n",
        "    config = generation_config.copy()\n",
        "\n",
        "    # Override parameters\n",
        "    config[\"generator\"] = torch.Generator(device=device).manual_seed(seed)\n",
        "    config[\"controlnet_conditioning_scale\"] = controlnet_scale\n",
        "    config[\"guidance_scale\"] = guidance\n",
        "\n",
        "    # Use custom prompt if provided\n",
        "    if outfit_prompt:\n",
        "        config[\"prompt\"] = outfit_prompt\n",
        "\n",
        "    # Update IP-Adapter scale if loaded\n",
        "    if IP_ADAPTER_LOADED:\n",
        "        pipe.set_ip_adapter_scale(ip_scale)\n",
        "        # Ensure the face image is in the config\n",
        "        config[\"ip_adapter_image\"] = face_for_ip_adapter\n",
        "\n",
        "    # Generate the image\n",
        "    result = pipe(**config)\n",
        "    return result.images[0]\n",
        "\n",
        "print(\"‚úÖ Variation helper function defined\")\n",
        "print(\"\"\"\n",
        "Usage examples:\n",
        "    # Generate with different seed\n",
        "    img = generate_variation(seed=123)\n",
        "\n",
        "    # Generate with different outfit\n",
        "    img = generate_variation(seed=42, outfit_prompt=\"person wearing a red dress\")\n",
        "\n",
        "    # Adjust identity preservation\n",
        "    img = generate_variation(seed=42, ip_scale=0.8)  # Stronger identity\n",
        "    img = generate_variation(seed=42, ip_scale=0.4)  # Weaker identity, better outfit\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fu8_nM1ZF09K"
      },
      "outputs": [],
      "source": [
        "# Generate 3 variations with different seeds\n",
        "print(\"üé≤ Generating variations with different seeds...\")\n",
        "variations = []\n",
        "seeds = [42, 123, 456]\n",
        "\n",
        "for seed in seeds:\n",
        "    print(f\"   Generating with seed {seed}...\")\n",
        "    var_img = generate_variation(seed)\n",
        "    variations.append(var_img)\n",
        "    var_img.save(OUTPUT_DIR / f\"variation_seed_{seed}.png\")\n",
        "\n",
        "print(\"‚úÖ Variations complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JBVi_tcF09K"
      },
      "outputs": [],
      "source": [
        "# Display all variations\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 10))\n",
        "\n",
        "for i, (img, seed) in enumerate(zip(variations, seeds)):\n",
        "    axes[i].imshow(img)\n",
        "    axes[i].set_title(f\"Variation (Seed: {seed})\", fontsize=14)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle(\"üé≤ Generated Variations\", fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / \"variations.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdNYSj0VF09K"
      },
      "source": [
        "## üéÅ Bonus: Try Different Outfits\n",
        "\n",
        "Test with different clothing descriptions to verify the system works for various outfits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9N6bwKnAF09K"
      },
      "outputs": [],
      "source": [
        "# Different outfit prompts to test\n",
        "outfits = {\n",
        "    \"black_dress\": \"\"\"A photorealistic full-body shot of a young woman wearing an elegant black midi dress,\n",
        "    black high heels, standing in a bright modern studio, natural lighting, 8k resolution, masterpiece\"\"\",\n",
        "\n",
        "    \"casual_summer\": \"\"\"A photorealistic full-body shot of a young woman wearing a floral summer dress,\n",
        "    white sandals, standing in a bright sunny studio, natural lighting, 8k resolution, masterpiece\"\"\",\n",
        "\n",
        "    \"business_casual\": \"\"\"A photorealistic full-body shot of a young woman wearing a beige blazer,\n",
        "    white blouse, black dress pants, black heels, standing in a modern office setting, 8k resolution, masterpiece\"\"\",\n",
        "\n",
        "    \"sporty\": \"\"\"A photorealistic full-body shot of a young woman wearing a navy blue sports hoodie,\n",
        "    black yoga pants, white running shoes, standing in a modern gym, natural lighting, 8k resolution, masterpiece\"\"\"\n",
        "}\n",
        "\n",
        "print(\"Available test outfits:\")\n",
        "for name, prompt in outfits.items():\n",
        "    print(f\"  üè∑Ô∏è {name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41cMncldF09K"
      },
      "outputs": [],
      "source": [
        "# Uncomment to generate an alternative outfit\n",
        "# outfit_name = \"black_dress\"\n",
        "# print(f\"üé® Generating '{outfit_name}' outfit...\")\n",
        "# alt_result = generate_variation(seed=42, outfit_prompt=outfits[outfit_name])\n",
        "# alt_result.save(OUTPUT_DIR / f\"{outfit_name}.png\")\n",
        "#\n",
        "# plt.figure(figsize=(8, 12))\n",
        "# plt.imshow(alt_result)\n",
        "# plt.title(f\"Alternative Outfit: {outfit_name}\", fontsize=14)\n",
        "# plt.axis('off')\n",
        "# plt.show()\n",
        "# print(f\"‚úÖ Saved to: {OUTPUT_DIR / f'{outfit_name}.png'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4bqe2EBF09K"
      },
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ POC Complete!\n",
        "\n",
        "### üìä Summary\n",
        "\n",
        "This proof of concept demonstrates:\n",
        "1. **Identity Preservation** using IP-Adapter with face embeddings from InsightFace\n",
        "2. **Pose Control** using ControlNet with OpenPose skeleton detection\n",
        "3. **Outfit Generation** using descriptive prompts with SDXL 1.0\n",
        "\n",
        "### üìÅ Files Generated\n",
        "- `outputs/result.png` - Main generated image\n",
        "- `outputs/pose_map.png` - Extracted pose skeleton\n",
        "- `outputs/comparison.png` - Side-by-side comparison\n",
        "- `outputs/variations.png` - Multiple seed variations\n",
        "\n",
        "Run the cell below to download all generated files!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roemQAThF09K"
      },
      "outputs": [],
      "source": [
        "# üì• Package all generated files\n",
        "import shutil\n",
        "\n",
        "# Create a zip of all outputs\n",
        "zip_path = shutil.make_archive('fitting_room_results', 'zip', OUTPUT_DIR)\n",
        "print(f\"üì¶ Results packaged: {zip_path}\")\n",
        "\n",
        "# Download for Google Colab users\n",
        "if USE_COLAB:\n",
        "    from google.colab import files\n",
        "    files.download('fitting_room_results.zip')\n",
        "    print(\"üì• Download started!\")\n",
        "else:\n",
        "    print(f\"\\nüìÇ All results saved in: {OUTPUT_DIR.resolve()}\")\n",
        "    print(f\"üì¶ Zip archive created: {Path(zip_path).resolve()}\")\n",
        "    print(\"\\nGenerated files:\")\n",
        "    for f in OUTPUT_DIR.glob(\"*\"):\n",
        "        print(f\"   - {f.name}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}